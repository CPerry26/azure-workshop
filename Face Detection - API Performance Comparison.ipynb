{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://img.evbuc.com/https%3A%2F%2Fcdn.evbuc.com%2Fimages%2F33408608%2F38309748108%2F1%2Foriginal.jpg?w=1000&rect=379%2C46%2C1366%2C683&s=8fc2ce5c141e04f89f8125cd6e24ec17)\n",
    "\n",
    "# Face Detection\n",
    "\n",
    "---------------------\n",
    "\n",
    "## A Proprietary API and Open-Source Performance Comparison\n",
    "\n",
    "How well do cloud service providers' deep learning models detect faces in images, really? How does that compare to an open-source package like OpenCV? And how much is that (potentially) improved accuracy and detail going to cost you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Available Features\n",
    "Below is a table of features from the top three cloud service providers, as of November 2017. These features and prices are subject to change without notice. A designation of `conf` indicates a confidence value of 0 to 1. \n",
    "\n",
    "_Also, note:_ Google's Vision API lists facial landmarks in three dimensions (x, y, z), whereas Microsoft's Face API, AWS Rekognition, and OpenCV only list two. For the OpenCV analysis, we will only be including out of the box XML files.\n",
    "\n",
    "-----------------------------------\n",
    "\n",
    "| Feature            | Google Vision API | AWS Rekognition | Microsoft Face API | OpenCV |\n",
    "| ------------------ |:-----------------:|:---------------:|:------------------:|:------:|\n",
    "| Custom Face Detect |                   |                 | X                  |        |\n",
    "| Bounding Box       | 2                 | 1               | 1                  | 1      |\n",
    "| Age                |                   | int range       | float + conf       |        |\n",
    "| Smile              | T/F               | T/F + conf      | conf               | conf   |\n",
    "| Eyeglasses         |                   | T/F + conf      | T/F + conf         |        |\n",
    "| Sunglasses         |                   | T/F + conf      | T/F + conf         |        |\n",
    "| Gender             |                   | M/F + conf      | M/F + conf         |        |\n",
    "| Hair Color         |                   |                 | color + conf       |        |\n",
    "| Makeup: Eye        |                   |                 | T/F + conf         |        |\n",
    "| Makeup: Lip        |                   |                 | T/F + conf         |        |\n",
    "| Hair: Bald         |                   |                 | conf               |        |\n",
    "| Hair: Invisible    |                   | T/F             | T/F + conf         |        |\n",
    "| Beard              |                   | T/F + conf      | T/F + conf         |        |\n",
    "| Moustache          |                   | T/F + conf      | T/F + conf         |        |\n",
    "| Sideburns          |                   |                 | T/F + conf         |        |\n",
    "| Eyes Open          |                   | T/F + conf      |                    |        |\n",
    "| Mouth Open         |                   | T/F + conf      |                    |        |\n",
    "| Eye Left           | X                 | X               | X                  |  X     |\n",
    "| Eye Right          | X                 | X               | X                  |  X     |\n",
    "| Left Pupil         | X                 | X               | X                  |        |\n",
    "| Right Pupil        | X                 | X               | X                  |        |\n",
    "| Nose Tip           | X                 | X               | X                  |        |\n",
    "| Nose Left          | X                 | X               | X                  |        |\n",
    "| Nose Right         | X                 | X               | X                  |        |\n",
    "| Nose Bottom Right  | X                 |                 | X                  |        |\n",
    "| Nose Bottom Left   | X                 |                 | X                  |        |\n",
    "| Nose Bottom Center | X                 |                 | X                  |        |\n",
    "| Upper Lip          | X                 |                 | X                  |        |\n",
    "| Lower Lip          | X                 |                 | X                  |        |\n",
    "| Upper Lip Top      |                   |                 | X                  |        |\n",
    "| Upper Lip Bottom   |                   |                 | X                  |        |\n",
    "| Lower Lip Top      |                   |                 | X                  |        |\n",
    "| Lower Lip Bottom   |                   |                 | X                  |        |\n",
    "| Mouth Left         | X                 | X               | X                  |        |\n",
    "| Mouth Right        | X                 | X               | X                  |        |\n",
    "| Mouth Center       | X                 |                 |                    |        | \n",
    "| Mouth Up           | X                 | X               |                    |        | \n",
    "| Mouth Down         | X                 | X               |                    |        | \n",
    "| Left Eyebrow Left  | X                 | X               | X                  |        |\n",
    "| Left Eyebrow Up    | X                 | X               |                    |        | \n",
    "| Left Eyebrow Right | X                 | X               | X                  |        |\n",
    "| Right Eyebrow Left | X                 | X               | X                  |        |\n",
    "| Right Eyebrow Up   | X                 | X               |                    |        |\n",
    "| Right Eyebrow Right| X                 | X               | X                  |        |\n",
    "| Eye Midpoint       | X                 |                 |                    |        | \n",
    "| Left Eye Left      | X                 | X               | X                  |        |\n",
    "| Left Eye Right     | X                 | X               | X                  |        |\n",
    "| Left Eye Top       | X                 | X               | X                  |        |\n",
    "| Left Eye Bottom    | X                 | X               | X                  |        |\n",
    "| Right Eye Left     | X                 | X               | X                  |        |\n",
    "| Right Eye Right    | X                 | X               | X                  |        |\n",
    "| Right Eye Top      | X                 | X               | X                  |        |\n",
    "| Right Eye Bottom   | X                 | X               | X                  |        |\n",
    "| Left Ear Tragion   | X                 |                 |                    |        |\n",
    "| Right Ear Tragion  | X                 |                 |                    |        |\n",
    "| Forehead Glabella  | X                 |                 |                    |        |\n",
    "| Chin Gnathion      | X                 |                 |                    |        |\n",
    "| Chin Left Gonion   | X                 |                 |                    |        |\n",
    "| Chin Right Gonion  | X                 |                 |                    |        |\n",
    "| Pitch              |                   |  X              |  X                 |        |\n",
    "| Yaw                |                   |  X              |  X                 |        |\n",
    "| Roll               | X                 |  X              |  X                 |        |\n",
    "| Pan                | X                 |                 |                    |        |\n",
    "| Tilt               | X                 |                 |                    |        |\n",
    "| Brightness         |                   |  % + conf       |                    |        |\n",
    "| Sharpness          |                   |  % + conf       | text + conf        |        |\n",
    "| Joy                | text              |   conf          | conf               |        |\n",
    "| Sorrow             | text              |                 | conf               |        |\n",
    "| Contempt           |                   |                 | conf               |        |\n",
    "| Disgust            |                   |                 | conf               |        |\n",
    "| Fear               |                   |                 | conf               |        |\n",
    "| Anger              | text              |   conf          | conf               |        |\n",
    "| Surprise           | text              |                 | conf               |        |\n",
    "| Confused           |                   |   conf          |                    |        |\n",
    "| Neutral            |                   |                 | conf               |        |\n",
    "| Exposure           | T/F               |                 | text + conf        |        |\n",
    "| Blur               | T/F               |                 | text + conf        |        |\n",
    "| Headwear           | T/F               |                 | X                  |        |\n",
    "| Occlusion          |                   |                 | T/F + conf         |        | |  \n",
    "\n",
    "Price structures, as of November 2017, are as follows (first 20 million transactions):\n",
    "\n",
    "![](http://paigesear.ch/api_costs.JPG)\n",
    "\n",
    "The graph above shows a monthly cost comparison for OpenCV, Amazon Web Services, Google Cloud Platform, and Microsoft's Azure Cognitive Services.\n",
    "\n",
    "Obviously, OpenCV (being open-source) has the lowest up-front cost; but you should also factor in what it would take to have a machine learning engineer or data scientist on staff who would be capable of building products, refining them, and supporting them. The same goes for deep learning frameworks: TensorFlow, CNTK, and Caffe2 would give results similar to the three proprietary APIs, but would be difficult to train, deploy as models, and maintain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facial Recognition - Full-Frontal Performance\n",
    "Our happy test subjects! Extra points if you can name all four.\n",
    "\n",
    "![](http://paigesear.ch/heroes.jpg)\n",
    "![](http://paigesear.ch/myotherdudes_analyzed.png)\n",
    "\n",
    "As you can see, all faces are detected by all listed tools - though there is some confusion around eye detection with OpenCV. It is also interesting to note that Amazon Web Services' Rekognition places bounding boxes at a tilt, as opposed to a square; this could potentially make custom algorithms or other libraries more difficult to implement.\n",
    "\n",
    "At any rate: detecting folks who are looking straight at the camera seems to be a bit too easy. Let's try something more challenging, shall we?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CoolPeopleHangingOutTogether.jpg\n",
    "\n",
    "![](coolpeoplehangingouttogether.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing JSON Files\n",
    "\n",
    "I've made calls to the three APIs listed above, using the picture shown. The responses are stored as `google`, `amazon`, and `microsoft`, respectively; and we will be using them for the remainder of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('C:\\\\Users\\\\pabailey\\\\Documents\\\\FaceAPITesting\\\\google.json') as goog_data:\n",
    "    google = json.load(goog_data)\n",
    "    #print(google)\n",
    "\n",
    "with open('C:\\\\Users\\\\pabailey\\\\Documents\\\\FaceAPITesting\\\\amazon.json') as aws_data:\n",
    "    amazon = json.load(aws_data)\n",
    "    #print(amazon)\n",
    "    \n",
    "with open('C:\\\\Users\\\\pabailey\\\\Documents\\\\FaceAPITesting\\\\microsoft.json') as azure_data:\n",
    "    microsoft = json.load(azure_data)\n",
    "    #print(microsoft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bounding Polygons\n",
    "It is a frustrating fact that none of the APIs return bounding box coordinates in a similar fashion; the closest are Google and Microsoft, with pixel locations. AWS Rekognition returns a proportion of the picture's height and width. The bounding boxes with a subset of the facial landmarks are shown below - and this time, the performance is much less than could be desired. Let's unpack it.\n",
    "\n",
    "_Note, again:_ AWS returns bounding polygons that are slightly askew. This could cause difficulty when creating custom facial algorithms, or attempting to integrate or standardize with other APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "// Exterior Polygons\n",
      "{'vertices': [{'x': 500, 'y': 223}, {'x': 689, 'y': 223}, {'x': 689, 'y': 412}, {'x': 500, 'y': 412}]}\n",
      "{'vertices': [{'x': 839, 'y': 282}, {'x': 1015, 'y': 282}, {'x': 1015, 'y': 459}, {'x': 839, 'y': 459}]}\n",
      "{'vertices': [{'x': 1184, 'y': 254}, {'x': 1392, 'y': 254}, {'x': 1392, 'y': 462}, {'x': 1184, 'y': 462}]}\n",
      "{'vertices': [{'x': 116, 'y': 233}, {'x': 319, 'y': 233}, {'x': 319, 'y': 436}, {'x': 116, 'y': 436}]}\n",
      "\n",
      " // Interior Polygons\n",
      "{'vertices': [{'x': 450, 'y': 161}, {'x': 696, 'y': 161}, {'x': 696, 'y': 447}, {'x': 450, 'y': 447}]}\n",
      "{'vertices': [{'x': 807, 'y': 190}, {'x': 1046, 'y': 190}, {'x': 1046, 'y': 468}, {'x': 807, 'y': 468}]}\n",
      "{'vertices': [{'x': 1168, 'y': 175}, {'x': 1445, 'y': 175}, {'x': 1445, 'y': 496}, {'x': 1168, 'y': 496}]}\n",
      "{'vertices': [{'x': 41, 'y': 128}, {'x': 331, 'y': 128}, {'x': 331, 'y': 465}, {'x': 41, 'y': 465}]}\n"
     ]
    }
   ],
   "source": [
    "# GoogleCloud Response\n",
    "print(\"// Exterior Polygons\")\n",
    "for i in range(4):\n",
    "    print(google['faceAnnotations'][i]['fdBoundingPoly'])\n",
    "\n",
    "print(\"\\n // Interior Polygons\")\n",
    "for i in range(4):\n",
    "    print(google['faceAnnotations'][i]['boundingPoly'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Width': 0.13437500596046448, 'Height': 0.18566493690013885, 'Left': 0.7174999713897705, 'Top': 0.21329879760742188}\n",
      "{'Width': 0.12812499701976776, 'Height': 0.17702935636043549, 'Left': 0.512499988079071, 'Top': 0.22452504932880402}\n",
      "{'Width': 0.12687499821186066, 'Height': 0.1744386851787567, 'Left': 0.31187498569488525, 'Top': 0.18134714663028717}\n"
     ]
    }
   ],
   "source": [
    "# AWS Rekognition\n",
    "for i in range(3):\n",
    "    print(amazon['FaceDetails'][i]['BoundingBox'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'top': 237, 'left': 521, 'width': 169, 'height': 169}\n",
      "{'top': 264, 'left': 188, 'width': 165, 'height': 165}\n",
      "{'top': 294, 'left': 840, 'width': 155, 'height': 155}\n"
     ]
    }
   ],
   "source": [
    "# Microsoft Cognitive Services\n",
    "for i in range(3):\n",
    "    print(microsoft[i]['faceRectangle'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenCV, after a bit of parameter tweaking, and AWS are able to detect everyone except for Claude Shannon; Azure is able to detect everyone except for Joseph Weizenbaum. Google's Vision API is the only tool that is able to detect features for all four, out of the box.\n",
    "\n",
    "_Note:_ the white and blue dots on faces for both AWS and Azure are just a subset of their available facial landmarks. For a full listing, see the table above.\n",
    "\n",
    "![](http://paigesear.ch/mydudes_analyzed.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Age Estimates\n",
    "_Note: only available from AWS and from Microsoft Azure._\n",
    "\n",
    "#### Correct ages of our dudes in April 1968:\n",
    "\n",
    "- Ed Fredkin: 34\n",
    "- Claude Shannon: 52\n",
    "- John McCarthy: 41\n",
    "- Joseph Weizenbaum:  45\n",
    "\n",
    "AWS Rekognition is the most spot-on for age estimates; however, the spreads are **huge** (20+ years for each of the detected faces). I assume the ranges equate to 95% confidence intervals, but that is not listed in documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon Rekognition - Age Detection\n",
      "{'Low': 38, 'High': 59} for John McCarthy\n",
      "{'Low': 35, 'High': 52} for Ed Fredkin\n",
      "{'Low': 48, 'High': 68} for Joseph Weizenbaum\n",
      "\n",
      "\n",
      "Microsoft Cognitive Services - Age Detection\n",
      "47.3 for Claude Shannon\n",
      "52.8 for John McCarthy\n",
      "39.4 for Ed Fredkin\n"
     ]
    }
   ],
   "source": [
    "print(\"Amazon Rekognition - Age Detection\")\n",
    "for i in range(3):\n",
    "    faces = [\"John McCarthy\", \"Ed Fredkin\", \"Joseph Weizenbaum\"]\n",
    "    print(amazon['FaceDetails'][i]['AgeRange'], \"for\", faces[i])\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Microsoft Cognitive Services - Age Detection\")\n",
    "for i in range(3):\n",
    "    faces = [\"Claude Shannon\", \"John McCarthy\", \"Ed Fredkin\"]\n",
    "    print(microsoft[i]['faceAttributes']['age'], \"for\", faces[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How happy is Ed Fredkin?\n",
    "God knows I'd be if I was standing next to Shannon, McCarthy, and Weizenbaum.\n",
    "\n",
    "![](http://paigesear.ch/EdFredkin.JPG)\n",
    "\n",
    "All three APIs agree that Ed's a pretty ecstatic guy; but the values are returned in quite dissimilar manners. Azure's Face API has top performance (99% confident in our test subject's happiness), while AWS is 52 - 62% confident that Ed is also disgusted and confused. Google's Vision API merely returns a `VERY_LIKELY`, with no corresponding confidence level.\n",
    "\n",
    "It should also be noted that the `SMILE` feature from Google, Microsoft, and AWS could merely look at positions for facial landmarks, and detect whether the two side corners of a subject's mouth are lifted about the upper lip's location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'anger': 0.0, 'contempt': 0.0, 'disgust': 0.0, 'fear': 0.0, 'happiness': 0.99, 'neutral': 0.009, 'sadness': 0.0, 'surprise': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# Microsoft Face API\n",
    "print(microsoft[2]['faceAttributes']['emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERY_LIKELY\n",
      "VERY_UNLIKELY\n",
      "VERY_UNLIKELY\n",
      "VERY_UNLIKELY\n"
     ]
    }
   ],
   "source": [
    "# Google Cloud Platform - Vision API\n",
    "print(google['faceAnnotations'][1]['joyLikelihood'])\n",
    "print(google['faceAnnotations'][1]['sorrowLikelihood'])\n",
    "print(google['faceAnnotations'][1]['surpriseLikelihood'])\n",
    "print(google['faceAnnotations'][1]['angerLikelihood'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Type': 'HAPPY', 'Confidence': 99.95712280273438}, {'Type': 'CONFUSED', 'Confidence': 0.6194353103637695}, {'Type': 'DISGUSTED', 'Confidence': 0.5203723907470703}]\n"
     ]
    }
   ],
   "source": [
    "# Amazon Rekognition\n",
    "print(amazon['FaceDetails'][1]['Emotions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Facial Landmarks and Orientation\n",
    "\n",
    "Google's Vision API lists all of its facial landmarks in three dimensions rather than two - which is excellent! The third dimension for AWS Rekognition and Microsoft's Face API could be derived with vector calculus, however (a combination of pitch, yaw, and known facial landmark locations).\n",
    "\n",
    "All three proprietary APIs place facial landmarks in similar locations; OpenCV often struggles with placement for eyes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': 535.6, 'y': 267.0}\n",
      "{'x': 566.3, 'y': 283.6}\n",
      "{'pitch': 0.0, 'roll': -1.6, 'yaw': 18.3}\n",
      "{'pitch': 0.0, 'roll': 0.0, 'yaw': 3.3}\n",
      "{'pitch': 0.0, 'roll': 2.7, 'yaw': -19.5}\n"
     ]
    }
   ],
   "source": [
    "print(microsoft[0]['faceLandmarks']['eyebrowLeftOuter'])\n",
    "print(microsoft[0]['faceLandmarks']['pupilLeft'])\n",
    "print(microsoft[0]['faceAttributes']['headPose'])\n",
    "print(microsoft[1]['faceAttributes']['headPose'])\n",
    "print(microsoft[2]['faceAttributes']['headPose'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'LEFT_OF_LEFT_EYEBROW', 'position': {'x': 547.7276, 'y': 267.71582, 'z': -3.303433}}\n",
      "{'type': 'LEFT_EYE', 'position': {'x': 570.0902, 'y': 283.44037, 'z': -0.0011843754}}\n",
      "-1.5004579\n",
      "29.29505\n",
      "5.0191813\n",
      "-1.1626552\n",
      "-8.133945\n",
      "-17.181002\n",
      "-3.3079503\n",
      "-59.581093\n",
      "-4.2570724\n",
      "8.72574\n",
      "69.301544\n",
      "-12.04517\n"
     ]
    }
   ],
   "source": [
    "print(google['faceAnnotations'][0]['landmarks'][2])\n",
    "print(google['faceAnnotations'][0]['landmarks'][0])\n",
    "print(google['faceAnnotations'][0]['rollAngle'])\n",
    "print(google['faceAnnotations'][0]['panAngle'])\n",
    "print(google['faceAnnotations'][0]['tiltAngle'])\n",
    "print(google['faceAnnotations'][1]['rollAngle'])\n",
    "print(google['faceAnnotations'][1]['panAngle'])\n",
    "print(google['faceAnnotations'][1]['tiltAngle'])\n",
    "print(google['faceAnnotations'][2]['rollAngle'])\n",
    "print(google['faceAnnotations'][2]['panAngle'])\n",
    "print(google['faceAnnotations'][2]['tiltAngle'])\n",
    "print(google['faceAnnotations'][3]['rollAngle'])\n",
    "print(google['faceAnnotations'][3]['panAngle'])\n",
    "print(google['faceAnnotations'][3]['tiltAngle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Type': 'leftEyeBrowLeft', 'X': 0.7546904683113098, 'Y': 0.26113489270210266}\n"
     ]
    }
   ],
   "source": [
    "print(amazon['FaceDetails'][0]['Landmarks'][7])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
